{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64987dc5",
   "metadata": {},
   "source": [
    "# YellowPage\n",
    "- yellowpage.ca\n",
    "\n",
    "    - Name. (must)\n",
    "    - Location.(must) --city, province, \n",
    "    - Website.(must)\n",
    "    - Email.(must)\n",
    "    - Telephone.(must)\n",
    "    - Sales. (opt)\n",
    "    - Staff (opt).\n",
    "    - Fax.(must)\n",
    "    - Established. (opt)\n",
    "    - Products/Notes/About.(must)\n",
    "    - Address.(opt)\n",
    "    - Description/business activity. (opt)\n",
    "    - NAICS (must)\n",
    "    - industry (must)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a9d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, httplib2, json, fire, re, string, requests\n",
    "from collections import OrderedDict, deque\n",
    "import re, requests, requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option(\"display.max.rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ac0949",
   "metadata": {
    "code_folding": [
     2,
     20,
     30,
     38,
     83,
     104
    ]
   },
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "    \n",
    "def get_tag_elements(url, tag='h2'):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    response = simple_get(url)\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        names = set()\n",
    "        for li in html.select(tag):\n",
    "            for name in li.text.split('\\n'):\n",
    "                if len(name) > 0:\n",
    "                    names.add(name.strip())\n",
    "        return list(names)\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url)) \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade69f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a2c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ad1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01452933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "import unicodecsv as csv\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "def parameters():\n",
    "    \"\"\"\n",
    "    Function to set parameters for search on yellow pages. Set parameter to True to include in search. Must have\n",
    "    at least one true for each parameter\n",
    "\n",
    "    \"\"\"\n",
    "    keyword_dict = {\n",
    "        'clothes': True\n",
    "    }\n",
    "\n",
    "    city_province_dict = {\n",
    "        'Toronto+ON': True\n",
    "    }\n",
    "\n",
    "    if sum(keyword_dict.values()) == 0 or sum(city_province_dict.values()) == 0:\n",
    "        sys.exit(\"Ensure you have specified at least one keyword and city, province\")\n",
    "    else:\n",
    "        return keyword_dict, city_province_dict\n",
    "\n",
    "def parse_listing(keyword, place_city_province, pagination=1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Function to process yellowpage listing page \n",
    "    : param keyword: search query\n",
    "    : param place_city : place city name\n",
    "    : param place_province: place province name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.yellowpages.ca/search/si/{2}/{0}/{1}\".format(keyword, place_city_province, pagination)\n",
    "    # /pagenumber/keyword/placecity+placeprovince\n",
    "\n",
    "    print(\"retrieving \",url)\n",
    "\n",
    "    headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "                'Accept-Encoding':'gzip, deflate, br',\n",
    "                'Accept-Language':'en-GB,en;q=0.9,en-US;q=0.8,ml;q=0.7',\n",
    "                'Cache-Control':'max-age=0',\n",
    "                'Connection':'keep-alive',\n",
    "                'Host':'www.yellowpages.ca',\n",
    "                'Upgrade-Insecure-Requests':'1',\n",
    "                'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36'\n",
    "            }\n",
    "\n",
    "    # Adding retries\n",
    "    for retry in range(100):\n",
    "        try:\n",
    "            response = requests.get(url,verify=False, headers = headers )\n",
    "\n",
    "            if response.status_code==200:\n",
    "                parser = html.fromstring(response.text)\n",
    "\n",
    "                #making links absolute\n",
    "                base_url = \"https://www.yellowpages.ca\"\n",
    "                parser.make_links_absolute(base_url)\n",
    "\n",
    "                XPATH_LISTINGS = \"//div[contains(@class, 'listing listing--bottomcta placement')]\" #listings\n",
    "                listings = parser.xpath(XPATH_LISTINGS)\n",
    "                scraped_results = []\n",
    "\n",
    "                for results in listings:\n",
    "\n",
    "                    XPATH_BUSINESS_NAME = \".//a[contains(@class, 'listing__name--link')]//text()\"\n",
    "                    XPATH_BUSINESS_PAGE = \".//a[contains(@class, 'listing__name--link')]//@href\"\n",
    "                    XPATH_STREET = \".//div[contains(@class, 'listing__address')]//span[@itemprop='address']//span[@itemprop='streetAddress']//text()\"\n",
    "                    XPATH_LOCALITY = \".//div[contains(@class, 'listing__address')]//span[@itemprop='address']//span[@itemprop='addressLocality']//text()\"\n",
    "                    XPATH_REGION = \".//div[contains(@class, 'listing__address')]//span[@itemprop='address']//span[@itemprop='addressRegion']//text()\"\n",
    "                    XPATH_ZIP_CODE = \".//div[contains(@class, 'listing__address')]//span[@itemprop='address']//span[@itemprop='postalCode']//text()\"\n",
    "                    XPATH_WEBSITE = \".//div[contains(@class, 'listing__mlr__root')]//ul/li[4]//@href\"\n",
    "\n",
    "                    raw_business_name = results.xpath(XPATH_BUSINESS_NAME)\n",
    "                    raw_business_page = results.xpath(XPATH_BUSINESS_PAGE)\n",
    "                    raw_website = results.xpath(XPATH_WEBSITE)\n",
    "                    raw_street = results.xpath(XPATH_STREET)\n",
    "                    raw_locality = results.xpath(XPATH_LOCALITY)\n",
    "                    raw_region = results.xpath(XPATH_REGION)\n",
    "                    raw_zip_code = results.xpath(XPATH_ZIP_CODE)\n",
    "                    \n",
    "                    business_name = ''.join(raw_business_name).strip() if raw_business_name else None\n",
    "                    business_page = ''.join(raw_business_page).strip() if raw_business_page else None\n",
    "                    street = ''.join(raw_street).strip() if raw_street else None\n",
    "                    website = ''.join(raw_website).strip() if raw_website else None\n",
    "                    locality = ''.join(raw_locality).replace(',\\xa0','').strip() if raw_locality else None\n",
    "                    region = ''.join(raw_region).strip() if raw_region else None\n",
    "                    zipcode = ''.join(raw_zip_code).strip() if raw_zip_code else None\n",
    "\n",
    "                    business_details = {\n",
    "                                        'business_name':business_name,\n",
    "                                        'business_page':business_page,\n",
    "                                        'street': street,\n",
    "                                        'website':website,\n",
    "                                        'street':street,\n",
    "                                        'locality':locality,\n",
    "                                        'region':region,\n",
    "                                        'zipcode':zipcode,\n",
    "                                        'search_category': keyword,\n",
    "                                        'updated_at': datetime.datetime.now()\n",
    "                    }\n",
    "                    scraped_results.append(business_details)\n",
    "\n",
    "                return scraped_results\n",
    "\n",
    "            elif response.status_code==404:\n",
    "                print(\"Could not find a location matching\",place)\n",
    "                #no need to retry for non existing page\n",
    "                break\n",
    "            else:\n",
    "                print(\"Failed to process page\")\n",
    "                return []\n",
    "                \n",
    "        except:\n",
    "            print(\"Failed to process page\")\n",
    "            return []\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    keyword_dict, place_city_province_dict = parameters()\n",
    "\n",
    "    keyword_list = [key for key, value in keyword_dict.items() if value]\n",
    "    place_city_province_list = [key for key, value in place_city_province_dict.items() if value]\n",
    "\n",
    "    sets = []\n",
    "\n",
    "    for place_city_province in place_city_province_list:\n",
    "        for keyword in keyword_list:\n",
    "            \n",
    "            sets.append(parse_listing(keyword,place_city_province)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9fd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e92f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86161b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1fb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:upwork]",
   "language": "python",
   "name": "conda-env-upwork-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
