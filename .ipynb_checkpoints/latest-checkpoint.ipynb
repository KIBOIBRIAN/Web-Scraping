{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d175ae69",
   "metadata": {},
   "source": [
    "# Companies in Canada\n",
    "\n",
    "- 1 million companies lines.\n",
    "\n",
    "**Must:** \n",
    "1. ~~Business Name~~\n",
    "2. ~~Primary Contact phone~~\n",
    "3. ~~Mailing address~~\n",
    "5. ~~Mailing city ~~\n",
    "6. ~~Website~~\n",
    "7. ~~Mailing Province~~\n",
    "8. ~~Business activity~~\n",
    "9. ~~Mailing Postal~~\n",
    "8. ~~Industry~~ -- we hard code this.\n",
    "9. ~~NAICS primary~~ -- we hard code this.\n",
    "4. ~~Fax~~ == phone\n",
    "11. Primary Contact name and title\n",
    "13. Primary Contact email\n",
    "14. Product /services\n",
    "\n",
    "**Optional:** \\\n",
    "15. Number of employees \\\n",
    "16. ~~Physical address~~ \\\n",
    "17. ~~Physical city \\~~\n",
    "18. Year founded \\\n",
    "19. ~~Physical Province \\~~\n",
    "20. ~~Physical Postal \\~~\n",
    "21. Sales volume \\\n",
    "22. SIC primary \n",
    "\n",
    "\n",
    "**Missing**:\n",
    "- Primary Contact email\n",
    "- Primary Contact name\n",
    "- Fax\n",
    "\n",
    "- Suggestion: pull email from website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0832dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrap as src\n",
    "\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from urllib.parse import urlsplit\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, httplib2, json, fire, re, string, requests\n",
    "from collections import OrderedDict, deque\n",
    "import re, requests, requests.exceptions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option(\"display.max.rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5521b6",
   "metadata": {},
   "source": [
    "# Yellow Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yelllowPages():\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    \n",
    "    def fetchAttributes(self):\n",
    "        \n",
    "        # title\n",
    "        titles = src.get_elements(self.url, \n",
    "                       search={\"class\": \"listing__name jsMapBubbleName\"},\n",
    "                 tag='h3'\n",
    "                )\n",
    "        titles = [i for i in titles if i.isdigit() == False][:-1]\n",
    "        \n",
    "        # activity: revisit the length\n",
    "        activities = src.get_elements(self.url, \n",
    "                 tag='article',\n",
    "                 search = {\"class\": \"listing__details__teaser\"}\n",
    "                )\n",
    "        activities = [i for i in activities if i !=\"See more text\"]\n",
    "        activities = [i for i in activities if i !=\"more...\"]\n",
    "        \n",
    "        # website: revisit length and alignment\n",
    "        def get_link(url):\n",
    "            parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                if \"/gourl/\" in link['href']:\n",
    "                    links.append(link['href'])\n",
    "\n",
    "            return links\n",
    "        sites = get_link(self.url)\n",
    "        sites = [i.split(\"%3A%2F%2F\")[-1].split(\"%\")[0] for i in sites]\n",
    "        \n",
    "        # physical & mailing addresses\n",
    "        def address_in4s(url):\n",
    "    \n",
    "            parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "            addresses = soup.find_all(\"span\", class_=\"jsMapBubbleAddress\")\n",
    "            addresses = [addresses[i: i + 4] for i in range(0, len(addresses), 4)]\n",
    "\n",
    "            return addresses\n",
    "        \n",
    "        addresses = address_in4s(self.url)\n",
    "        listed = []\n",
    "        for j in range(len(addresses)):\n",
    "            listed.append([i.getText() for i in addresses[j]])\n",
    "\n",
    "        Mailing_address = []\n",
    "        Mailing_city = []\n",
    "        Mailing_Province = []\n",
    "        Mailing_Postal = []\n",
    "        Physical_address = []\n",
    "        Physical_city = []\n",
    "\n",
    "        for i in range(len(listed)):\n",
    "            Mailing_address.append(listed[i][0])\n",
    "            Physical_address.append(listed[i][0])\n",
    "            Mailing_city.append(listed[i][1])  \n",
    "            Physical_city.append(listed[i][1])    \n",
    "            Mailing_Province.append(listed[i][2])    \n",
    "            Mailing_Postal.append(listed[i][3])   \n",
    "            \n",
    "        # phones + fax\n",
    "        def get_phone(url):\n",
    "            source_code = requests.get(url)\n",
    "            plain_text = source_code.content\n",
    "            soup = BeautifulSoup(plain_text, \"lxml\")\n",
    "            links = soup.findAll('a', {'class': 'mlr__item__cta jsMlrMenu'})\n",
    "\n",
    "            phones = []\n",
    "            for link in links:\n",
    "                title = link.get('data-phone')\n",
    "                phones.append(title)\n",
    "\n",
    "            return phones\n",
    "    \n",
    "        phones = get_phone(self.url)\n",
    "        fax = phones\n",
    "        \n",
    "        # emails\n",
    "        def get_email(url):\n",
    "\n",
    "            unprocessed_urls = deque([url])\n",
    "            processed_urls = set()\n",
    "            emails = set()\n",
    "            url = url\n",
    "\n",
    "            print(\"Crawling URL %s\" % url)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I))\n",
    "            new_emails = [mail for mail in new_emails if \".png\" not in mail]\n",
    "            new_emails = [mail for mail in new_emails if \".jpg\" not in mail]\n",
    "            new_emails = [i for i in new_emails if len(i) < 30]\n",
    "\n",
    "            emails.update(new_emails)\n",
    "\n",
    "            return emails\n",
    "\n",
    "        mails = []\n",
    "        for site in [i.split(\"%3A%2F%2F\")[-1].split(\"%\")[0] for i in get_link(self.url)]:\n",
    "            try:\n",
    "                mail = get_email(\"http://\" + site)\n",
    "\n",
    "            except Exception:\n",
    "                try:\n",
    "                    mail = get_email(\"https://\" + site)\n",
    "\n",
    "                except Exception:\n",
    "                    mail = \"null\"\n",
    "                    pass\n",
    "\n",
    "            mails.append(mail)\n",
    "            \n",
    "        # products and services\n",
    "        pass\n",
    "    \n",
    "        return titles, activities, sites, phone, fax, mails, Mailing_address, Mailing_city,\n",
    "    Mailing_Province, Mailing_Postal, Physical_address, Physical_city\n",
    "    \n",
    "    def create_df():\n",
    "        df = pd.DataFrame(columns = [\n",
    "            \"name\", \"phone\", \"activity\", \"website\", \"mail\", \"mail_address\",\n",
    "            \"mail_city\", \"mail_province\", \"fax\", \"mail_postal\", \"physical_address\", \"physical_city\"\n",
    "        ])\n",
    "        df[\"name\"], df[\"activity\"], df[\"website\"], df[\"phone\"], df[\"fax\"], df[\"name\"], df[\"name\"], \n",
    "        df[\"name\"], df[\"name\"], df[\"name\"], df[\"name\"], df[\"name\"],  = fetchAttributes(self.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614ab13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a250929d",
   "metadata": {},
   "source": [
    "## Loop page nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee325a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    try:\n",
    "        pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "# add industry and NAICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0c620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae08ca61",
   "metadata": {},
   "source": [
    "5. Position mixup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f24ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72b1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418bdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128bfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3edd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593d4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470f12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74ea1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f22991fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "# resp = urllib.request.urlopen(\"https://www.yellowpages.ca/search/si/1/Restaurants/Toronto+ON\")\n",
    "# soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "# links = []\n",
    "# for link in soup.find_all('a', href=True):\n",
    "#     if \"bus/\" in link['href']:\n",
    "#         if \"#\" not in link[\"href\"]:\n",
    "#             if \"Review\" not in link[\"href\"]:\n",
    "#                 links.append(link['href'])\n",
    "\n",
    "\n",
    "\n",
    "# links = list(OrderedDict.fromkeys(links))\n",
    "# links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c27e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c49f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c3aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
